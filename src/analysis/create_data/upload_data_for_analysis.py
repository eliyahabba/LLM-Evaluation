import argparse
import logging
import os
import time
from datetime import datetime
from multiprocessing import Pool, cpu_count, Manager
from pathlib import Path
from functools import partial

from huggingface_hub import (
    HfApi,
    create_repo,
    CommitOperationAdd,
    create_commit
)
from tqdm import tqdm

from config.get_config import Config

def setup_logging():
    """
    הגדרת לוגים בסיסית עם תאריך ושעה, כתיבה לקובץ + הדפסה למסך
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - [PID %(process)d] - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'processing_log_{timestamp}.log'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


def chunkify(lst, chunk_size):
    """
    מחלקת רשימה (lst) לצ'אנקים בגודל chunk_size (גנרטור).
    """
    for i in range(0, len(lst), chunk_size):
        yield lst[i:i+chunk_size]


def upload_chunk_files(args):
    """
    פונקציית העלאה המתבצעת עבור צ'אנק אחד.
    פרמטרים (args) מגיעים כטאפ"ל:
    (chunk_index, chunk_files, repo_name, token, logger)
    """
    chunk_index, chunk_files, repo_name, token, logger = args
    logger.info(f"Uploading chunk #{chunk_index} with {len(chunk_files)} files...")

    try:
        operations = []
        for local_file in chunk_files:
            operations.append(
                CommitOperationAdd(
                    path_in_repo=os.path.basename(local_file),  # שם הקובץ ב-HF
                    path_or_fileobj=local_file
                )
            )

        # יוצרים commit יחיד עם כל הקבצים שבצ'אנק
        create_commit(
            repo_id=repo_name,
            repo_type="dataset",
            operations=operations,
            commit_message=f"Upload chunk {chunk_index}",
            token=token
        )

        logger.info(f"Successfully uploaded chunk #{chunk_index}")
    except Exception as e:
        logger.error(f"Error uploading chunk #{chunk_index}: {e}")


def upload_files_in_chunks(process_output_dir, repo_name, token, chunk_size=5):
    """
    - אוספת את רשימת הקבצים בתיקייה process_output_dir.
    - מחלקת לצ'אנקים (קבוצות של chunk_size).
    - מריצה העלאה מקבילית לכל צ'אנק.
    """
    logger = setup_logging()
    logger.info("Starting chunked upload...")

    # 1. יוצרים Repo אם לא קיים
    api = HfApi()
    try:
        create_repo(repo_id=repo_name, token=token, private=True, repo_type="dataset")
        logger.info(f"Repository '{repo_name}' created successfully.")
    except Exception as e:
        logger.info(f"Repository '{repo_name}' might already exist or error occurred: {e}")

    # 2. אוספים את כל הקבצים בתיקייה
    all_files = []
    for fname in os.listdir(process_output_dir):
        fpath = os.path.join(process_output_dir, fname)
        if os.path.isfile(fpath):
            all_files.append(fpath)

    logger.info(f"Total files found: {len(all_files)}")

    # 3. מחלקים לצ'אנקים
    chunked = list(chunkify(all_files, chunk_size))
    logger.info(f"Splitting files into {len(chunked)} chunks (chunk_size={chunk_size})")

    # 4. מקביליות - הגדרת כמות התהליכים
    num_processes = min(24, cpu_count() - 1)
    logger.info(f"Starting parallel upload with {num_processes} processes...")

    # יוצרים רשימת משימות
    tasks = []
    for idx, chunk_files in enumerate(chunked):
        tasks.append((idx, chunk_files, repo_name, token, logger))

    # 5. העלאה במקביל באמצעות Pool
    with Pool(processes=num_processes) as pool:
        for _ in tqdm(pool.imap_unordered(upload_chunk_files, tasks),
                      total=len(tasks),
                      desc="Uploading chunks"):
            pass

    logger.info("All chunks finished uploading.")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--chunk_size", type=int, default=5, help="Number of files per chunk")
    args = parser.parse_args()

    config = Config()
    TOKEN = config.config_values.get("hf_access_token", "")
    repo_name = "eliyahabba/llm-evaluation-analysis"

    process_output_dir = "/cs/snapless/gabis/eliyahabba/ibm_results_data_full_processed"
    os.makedirs(process_output_dir, exist_ok=True)

    upload_files_in_chunks(
        process_output_dir=process_output_dir,
        repo_name=repo_name,
        token=TOKEN,
        chunk_size=args.chunk_size
    )


if __name__ == "__main__":
    main()